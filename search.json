[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm Prashant Patel, an AI/ML Engineer with a love for building clean, thoughtful, and scalable systems. I‚Äôve worked across both consultancies and large product companies, which has given me a strong foundation in system integration, solution architecture, and the many hats an AI Engineer ends up wearing - be it MLOps, backend development, or designing intelligent solutions end-to-end.\nI approach software design like building with LEGO-structured, modular, and purposeful. To me, writing code is a quiet act of communication with the future; it should be as clear and intentional as a well-written message to the next developer who picks it up. I enjoy thinking at abstract levels and solving real-world problems with elegance and simplicity. üå±\nA couple of years ago, I started neuralware. - a name that reflects where I believe software is headed: systems powered not just by logic, but by learning. It‚Äôs rooted in a long-standing fascination with neural networks and the idea that software is evolving into something more organic, more adaptable.\nReading Building a Second Brain by Tiago Forte inspired me to begin sharing my knowledge through this blog. We now live in an era where AI models are trained on our digital footprints - our code, our thoughts, our documentation. I believe that to shape a truly insightful oracle for the future, we need to feed it new, thoughtful, and human content - not just recycled history. It struck me how timely it is to contribute something meaningful to the growing collective knowledge.\nI naturally learn by doing - constantly experimenting, absorbing new ideas, and refining my understanding. This process of creation and iteration resonates deeply with the idea that we only know what we make, as Giambattista Vico so eloquently put it:\n\n‚ÄúVerum Ipsum Factum‚Äù\n‚Äì Giambattista Vico, an Italian Philosopher\n\nOutside of work, you‚Äôll find me either chasing after my toddler, getting lost in a good book, or immersing myself in platformer games on my Switch. I‚Äôm not one for spending much time outdoors - unless there‚Äôs a mountain involved. While I‚Äôm no professional climber, I am always up for scaling heights with good company to enjoy the views from the top; I find it the best way to disconnect and refresh.\n\n\n\nClimbing Cat Bells with my wife (Lake District, Cumbria)"
  },
  {
    "objectID": "posts/context-engineering-part-2/index.html",
    "href": "posts/context-engineering-part-2/index.html",
    "title": "Context Engineering: Building Smarter AI Agents - Part 2/3",
    "section": "",
    "text": "Having established the critical role of context engineering in our previous post, let‚Äôs delve into how LangGraph provides the architectural framework to implement these principles effectively.\n\n\nLangGraph‚Äôs design inherently makes it an exceptional tool for context engineering:\n\nExplicit Control over Information Flow: The graph-based structure forces you to explicitly define how information (context) flows between different components of your application. You decide precisely what information enters each node and what information is passed on to the next. This eliminates implicit dependencies and makes context management transparent.\nFacilitates Complex, Multi-Step Reasoning: Many real-world tasks require more than a single LLM call. They involve multiple steps of reasoning, tool use, and information synthesis. LangGraph allows you to orchestrate these complex sequences, ensuring that the context is meticulously managed at each stage. For example, an agent might first retrieve documents, then summarise them, then use the summary to answer a question, and finally, if needed, perform a follow-up search ‚Äì all while maintaining a consistent and evolving context.\nSupports Multi-Agent Workflows: LangGraph is particularly well-suited for building multi-agent systems, where different specialised agents collaborate to achieve a common goal. In such systems, agents often need to share and update a common context. LangGraph‚Äôs shared state mechanism and routing capabilities make it straightforward for agents to pass information back and forth, ensuring that each agent has the necessary context to perform its specific role effectively.\n\nIn the next section, we will explore practical examples that demonstrate how these LangGraph concepts translate into robust context engineering solutions for common LLM application patterns."
  },
  {
    "objectID": "posts/context-engineering-part-2/index.html#langgraph-your-toolkit-for-context-engineering",
    "href": "posts/context-engineering-part-2/index.html#langgraph-your-toolkit-for-context-engineering",
    "title": "Context Engineering: Building Smarter AI Agents - Part 2/3",
    "section": "",
    "text": "Having established the critical role of context engineering in our previous post, let‚Äôs delve into how LangGraph provides the architectural framework to implement these principles effectively.\n\n\nLangGraph‚Äôs design inherently makes it an exceptional tool for context engineering:\n\nExplicit Control over Information Flow: The graph-based structure forces you to explicitly define how information (context) flows between different components of your application. You decide precisely what information enters each node and what information is passed on to the next. This eliminates implicit dependencies and makes context management transparent.\nFacilitates Complex, Multi-Step Reasoning: Many real-world tasks require more than a single LLM call. They involve multiple steps of reasoning, tool use, and information synthesis. LangGraph allows you to orchestrate these complex sequences, ensuring that the context is meticulously managed at each stage. For example, an agent might first retrieve documents, then summarise them, then use the summary to answer a question, and finally, if needed, perform a follow-up search ‚Äì all while maintaining a consistent and evolving context.\nSupports Multi-Agent Workflows: LangGraph is particularly well-suited for building multi-agent systems, where different specialised agents collaborate to achieve a common goal. In such systems, agents often need to share and update a common context. LangGraph‚Äôs shared state mechanism and routing capabilities make it straightforward for agents to pass information back and forth, ensuring that each agent has the necessary context to perform its specific role effectively.\n\nIn the next section, we will explore practical examples that demonstrate how these LangGraph concepts translate into robust context engineering solutions for common LLM application patterns."
  },
  {
    "objectID": "posts/context-engineering-part-2/index.html#practical-examples-of-context-engineering-with-langgraph",
    "href": "posts/context-engineering-part-2/index.html#practical-examples-of-context-engineering-with-langgraph",
    "title": "Context Engineering: Building Smarter AI Agents - Part 2/3",
    "section": "Practical Examples of Context Engineering with LangGraph",
    "text": "Practical Examples of Context Engineering with LangGraph\nNow, let‚Äôs dive into practical examples to illustrate how LangGraph facilitates robust context engineering. For these examples, we will use langchain and langgraph. Make sure you have these libraries installed (pip install langchain langchain-openai langgraph) and have your OpenAI API key set as an environment variable (OPENAI_API_KEY).\n\nStateful AI Assitant\nProblem: A common challenge in building conversational AI is maintaining context across multiple turns. A basic LLM call is stateless, meaning it forgets previous interactions. Furthermore, an AI Assistant often needs to access external information (e.g., current time, weather) to provide relevant responses.\nContext Engineering Solution with LangGraph: We can design a LangGraph application that maintains conversation history in its state and dynamically decides whether to use a tool based on the user‚Äôs query. The state will hold the chat history, which is crucial context for the LLM.\nConceptual Flow:\n\nUser Input: The user sends a message.\nLLM Call (with History): The LLM receives the current message along with the accumulated conversation history.\nConditional Edge (Tool Decision): The LLM decides if an external tool (e.g., a search tool) is needed to answer the query.\nTool Call (if needed): If a tool is required, it‚Äôs invoked, and its output is added to the context.\nLLM Call (with Tool Output): The LLM receives the original query, history, and now the tool‚Äôs output to formulate a final response.\nResponse: The LLM generates the final answer.\n\nLet‚Äôs implement a simplified version of this, focusing on maintaining history and a basic tool call.\nimport operator\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolNode\n\n\n# 1. Define the Graph State\n1class AgentState(TypedDict):\n    # The list of messages passed between the agents\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n\n\n# Define a simple tool\n@tool\n2def get_current_time(query: str) -&gt; str:\n    \"\"\"Returns the current time. Use this tool when asked about the current time.\"\"\"\n    import datetime\n\n    return str(datetime.datetime.now().strftime(\"%H:%M:%S\"))\n\n\n# Create tools list\ntools = [get_current_time]\n\n\n# 2. Define the Nodes\n3def call_llm(state: AgentState):\n    \"\"\"Node for LLM interaction with tool calling capabilities\"\"\"\n    messages = state[\"messages\"]\n    model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\n\n# Use LangGraph's built-in ToolNode for real tool execution\n4tool_node = ToolNode(tools)\n\n\n# 3. Define the Conditional Edge Logic\n5def should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # Check if the last message has tool calls\n    if last_message.tool_calls:\n        return \"call_tool\"\n    return \"end\"\n\n\n6# 4. Build the Graph\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"llm\", call_llm)\nworkflow.add_node(\"tool\", tool_node)\n\nworkflow.set_entry_point(\"llm\")\n\n# Add conditional edges based on tool calls\nworkflow.add_conditional_edges(\n    \"llm\",\n    should_continue,\n    {\n        \"call_tool\": \"tool\",\n        \"end\": END,\n    },\n)\n\n# After tool execution, always return to LLM to process results\nworkflow.add_edge(\"tool\", \"llm\")\n\napp = workflow.compile()\n\n# 5. Run the AI Assistant\nprint(\"\\n--- AI Assistant Interaction ---\")\ninputs = {\"messages\": [HumanMessage(content=\"Hello, how are you?\")]}\nfor s in app.stream(inputs):\n    print(s)\n\nprint(\"\\n--- AI Assistant Interaction (asking for time) ---\")\ninputs = {\"messages\": [HumanMessage(content=\"What is the current time?\")]}\nfor s in app.stream(inputs):\n    print(s)\n\n1\n\nAgentState is a TypedDict that defines the structure of our graph‚Äôs state. It primarily holds a messages list, which is crucial for maintaining conversation history. The operator.add annotation ensures that new messages are appended to the existing list, preserving the context.\n\n2\n\nget_current_time tool is a simple Python function decorated with @tool that simulates an external capability. The LLM can be prompted to use this tool.\n\n3\n\ncall_llm node takes the current messages from the state, invokes the ChatOpenAI model, and returns the LLM‚Äôs response, which is then added back to the messages in the state.\n\n4\n\ntool_node is responsible for executing the tool.\n\n5\n\nshould_continue conditional edge determines the next step based on the LLM‚Äôs last message. If the LLM indicates a tool call, it routes to the tool_node; otherwise, it routes to END (meaning the conversation turn is complete) or back to llm for further processing.\n\n6\n\nTo construct the graph we define the nodes and then use add_conditional_edges to create the dynamic flow. The set_entry_point defines where the graph execution begins.\n\n\nThis example demonstrates how LangGraph uses its State to manage the evolving context (conversation history) and Conditional Edges to dynamically route the flow based on the LLM‚Äôs decision, enabling tool use. The entire interaction, including the tool‚Äôs output, becomes part of the context for subsequent LLM calls.\n\n\nDocument Q&A with RAG\nProblem: Large Language Models have vast general knowledge, but they lack specific, up-to-date, or proprietary information contained within private documents (e.g., internal company policies, specific research papers). Directly feeding large documents into the LLM‚Äôs context window is often impractical due to token limits and can lead to the LLM getting lost in the noise.\nContext Engineering Solution with LangGraph: The solution is to implement a Retrieval Augmented Generation (RAG) pipeline. In a RAG system, we first retrieve relevant snippets of information from our documents based on the user‚Äôs query. Then, we provide these snippets as context to the LLM, along with the original query, to generate a concise and accurate answer. LangGraph is ideal for orchestrating this multi-step process.\nConceptual Flow:\n\nUser Query: The user asks a question about the documents.\nRetriever: A retriever (e.g., a vector database) searches the document collection and finds the most relevant chunks of text.\nLLM (Generation): The LLM receives the user‚Äôs query and the retrieved document snippets as context and generates an answer based on this information.\n\nLet‚Äôs build a simple RAG agent using LangGraph. We‚Äôll use a simple in-memory vector store for this example.\nfrom typing import List, TypedDict\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langgraph.graph import StateGraph, END\n\n# 1. Define the Graph State\n1class RagState(TypedDict):\n    query: str\n    documents: List[Document]\n    answer: str\n\n# 2. Set up the Retriever\n# Sample documents\ntexts = [\n    \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\",\n    \"Context engineering is the art of providing the right information to an LLM.\",\n    \"LangGraph makes it easy to create complex agentic workflows.\"\n]\ndocuments = [Document(page_content=t) for t in texts]\n\n# Create a simple in-memory vector store\nembeddings = OpenAIEmbeddings()\n2vectorstore = FAISS.from_documents(documents, embeddings)\nretriever = vectorstore.as_retriever()\n\n# 3. Define the Nodes\n3def retrieve_documents(state: RagState):\n    query = state[\"query\"]\n    retrieved_docs = retriever.invoke(query)\n    return {\"documents\": retrieved_docs}\n\n4def generate_answer(state: RagState):\n    query = state[\"query\"]\n    documents = state[\"documents\"]\n    \n    # Create a prompt template\n    prompt_template = \"\"\"Answer the user's question based only on the following \n    context:\\n\\n{context}\\n\\nQuestion: {question}\"\"\"\n    prompt = ChatPromptTemplate.from_template(prompt_template)\n    \n    # Create a chain\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n    \n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n    rag_chain = (\n        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n    )\n    \n    answer = rag_chain.invoke(query)\n    return {\"answer\": answer.content}\n\n5# 4. Build the Graph\nworkflow = StateGraph(RagState)\n\nworkflow.add_node(\"retriever\", retrieve_documents)\nworkflow.add_node(\"generator\", generate_answer)\n\nworkflow.set_entry_point(\"retriever\")\nworkflow.add_edge(\"retriever\", \"generator\")\nworkflow.add_edge(\"generator\", END)\n\napp = workflow.compile()\n\n# 5. Run the RAG Agent\nprint(\"\\n--- RAG Agent Interaction ---\")\ninputs = {\"query\": \"What is LangGraph?\"}\nresult = app.invoke(inputs)\nprint(f\"Query: {result['query']}\")\nprint(f\"Answer: {result['answer']}\")\n\n1\n\nThis state dictionary holds the query, the retrieved documents, and the final answer.\n\n2\n\nFor retriever, we create a simple in-memory vector store using FAISS from a few sample documents. In a real-world application, this would likely be a more robust, persistent vector database.\n\n3\n\nretrieve_documents node takes the user‚Äôs query from the state, uses the retriever to find relevant documents, and updates the documents field in the state.\n\n4\n\ngenerate_answer node constructs a prompt that includes the retrieved documents as context. It then invokes the LLM to generate an answer based on this context and updates the answer field in the state.\n\n5\n\nGraph Construction: This is a simpler, linear graph. We define the nodes and then create a straightforward flow from the retriever to the generator and finally to the END.\n\n\nThis RAG example showcases how LangGraph can be used to orchestrate a multi-step data processing pipeline. The context (retrieved documents) is explicitly passed from one node to the next via the shared State, ensuring that the LLM has the necessary information to generate a grounded and accurate answer.\n\n\nMulti-Agent Workflow\nProblem: Some tasks are too complex for a single LLM or a single agent to handle effectively. They might require different areas of expertise, or they might benefit from a divide-and-conquer approach. For example, generating a comprehensive research report might involve searching for information, analysing data, writing content, and editing the final draft.\nContext Engineering Solution with LangGraph: LangGraph is exceptionally well-suited for creating multi-agent workflows. We can define different agents as nodes in the graph and have them collaborate on a shared task. The shared State in LangGraph becomes the central scratchpad where agents can read the current status of the task, access the work of other agents, and contribute their own results.\nConceptual Flow:\n\nOrchestrator Agent: An orchestrator or manager agent receives the initial task and decomposes it into sub-tasks.\nSpecialised Sub-Agents: The orchestrator routes the sub-tasks to specialised agents (e.g., a research agent, a writing agent, an editing agent).\nShared State: The sub-agents perform their tasks and update the shared state with their results (e.g., research findings, written paragraphs, edited text).\nSynthesis: The orchestrator monitors the progress in the shared state and, once all sub-tasks are complete, synthesizes the results into a final output.\n\nLet‚Äôs create a simplified two-agent system: a Researcher Agent that finds information and a Writer Agent that uses that information to write a short paragraph.\nfrom typing import List, TypedDict, Annotated\nimport operator\n\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END\n\n\n# A simple search tool for the researcher\n@tool\ndef simple_search(query: str) -&gt; str:\n    \"\"\"A simple search tool that returns a predefined string for a given query.\"\"\"\n    if \"context engineering\" in query.lower():\n        return (\n            \"Context engineering is the practice \"\n            \"of designing and managing the information provided \"\n            \"to an LLM to improve its performance.\"\n        )\n    else:\n        return \"No information found.\"\n\n\n# 1. Define the Graph State\n1class MultiAgentState(TypedDict):\n    messages: Annotated[List[BaseMessage], operator.add]\n    sender: str\n\n\n# 2. Define the Agents (as nodes)\n2def researcher_agent(state: MultiAgentState):\n    # This agent's job is to use the search tool\n    query = state[\"messages\"][-1].content\n    tool_output = simple_search.invoke(query)\n    return {\"messages\": [AIMessage(content=tool_output)], \"sender\": \"Researcher\"}\n\n\ndef writer_agent(state: MultiAgentState):\n    # This agent's job is to write a paragraph based on the researcher's findings\n    research_finding = state[\"messages\"][-1].content\n    prompt = (\n        \"Write a short, engaging paragraph \"\n        f\"about the following topic: {research_finding}\"\n    )\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n    response = llm.invoke(prompt)\n    return {\"messages\": [AIMessage(content=response.content)], \"sender\": \"Writer\"}\n\n\n# 3. Define the Router (a conditional edge)\n3def router(state: MultiAgentState):\n    # This router decides which agent to send the message to next\n    sender = state[\"sender\"]\n    if sender == \"Researcher\":\n        return \"writer\"\n    else:\n        return \"researcher\"\n\n\n4# 4. Build the Graph\nworkflow = StateGraph(MultiAgentState)\n\nworkflow.add_node(\"researcher\", researcher_agent)\nworkflow.add_node(\"writer\", writer_agent)\n\n# The router will decide the first agent to call\nworkflow.add_conditional_edges(\n    \"__start__\",\n    lambda state: \"researcher\",  # Start with the researcher\n    {\"researcher\": \"researcher\"},\n)\n\nworkflow.add_conditional_edges(\"researcher\", router, {\"writer\": \"writer\"})\n\nworkflow.add_conditional_edges(\n    \"writer\",\n    lambda state: \"__end__\",  # End after the writer\n    {\"__end__\": END},\n)\n\napp = workflow.compile()\n\n# 5. Run the Multi-Agent System\nprint(\"\\n--- Multi-Agent System Interaction ---\")\ninputs = {\"messages\": [HumanMessage(content=\"Tell me about context engineering\")]}\nfor s in app.stream(inputs):\n    print(s)\n\n1\n\nMultiAgentState includes a sender field to track which agent last modified the state. This is crucial for routing.\n\n2\n\nresearcher_agent and writer_agent nodes are functions that represent our two specialised agents. The researcher uses the simple_search tool, and the writer uses an LLM to generate text based on the researcher‚Äôs findings.\n\n3\n\nrouter Conditional Edge is a function that acts as the central orchestrator. It inspects the sender in the state and decides which agent should act next. In this simple case, it creates a linear handoff from the researcher to the writer.\n\n4\n\nGraph Construction: We define the agent nodes and then use conditional edges to control the flow. We start with the researcher, then the router sends the control to the writer, and finally, the workflow ends.\n\n\nThis multi-agent example illustrates how LangGraph can be used to build complex, collaborative systems. The shared State acts as the communication channel and shared memory between agents, and the routing logic allows for sophisticated orchestration. This is a powerful paradigm for tackling complex problems that benefit from multiple specialised perspectives, all underpinned by careful context engineering."
  },
  {
    "objectID": "posts/context-engineering-part-2/index.html#conclusion",
    "href": "posts/context-engineering-part-2/index.html#conclusion",
    "title": "Context Engineering: Building Smarter AI Agents - Part 2/3",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we explored how LangGraph empowers context engineering through explicit control of information flow, multi-step reasoning, and seamless multi-agent collaboration. By walking through practical examples - stateful assistants, RAG pipelines, and multi-agent workflows - we‚Äôve seen how LangGraph‚Äôs architecture makes context management transparent and scalable for real-world LLM applications.\nIn the final part of this three-part series, we will summarise the context engineering best practices."
  },
  {
    "objectID": "posts/langgraph-deployment/index.html",
    "href": "posts/langgraph-deployment/index.html",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "",
    "text": "Building and deploying a LangGraph AI Agent from scratch involves understanding the framework‚Äôs architecture, defining your agent‚Äôs workflow as a graph, implementing nodes and state management, and finally deploying the agent either locally or on LangGraph Cloud. Below is a detailed guide covering all these steps.\nBefore we dive-in, let‚Äôs get ourselves familiarised with some components involved in developing and deploying a LangGraph application."
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#components",
    "href": "posts/langgraph-deployment/index.html#components",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Components",
    "text": "Components\n\nLangGraph is the foundational library enabling agent workflow creation in Python and JavaScript.\nLangGraph API wraps the graph logic, managing asynchronous tasks and state persistence, serving as the backend engine.\nLangGraph Cloud hosts the API, providing deployment, monitoring, and accessible endpoints for running graphs in production.\nLangGraph Studio is the development environment that leverages the API backend for real-time graph building and testing, usable locally or in the cloud.\nLangGraph SDK offers programmatic access to LangGraph graphs, abstracting whether the graph is local or cloud-hosted, facilitating client creation and workflow execution.\n\n\n\n\n\n\n\nFigure¬†1: LangGraph Components"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#set-up",
    "href": "posts/langgraph-deployment/index.html#set-up",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Set Up",
    "text": "Set Up\nWe‚Äôll be building a simple Agent to demonstrate the end-to-end process. Building more sophisticated AI agents is a topic better suited for a dedicated post.\nTo start with, create a following project structure and open langgraph_deployment directory in your favorite code editor.\nlanggraph_deployment/\n‚îú‚îÄ‚îÄsrc\n‚îÇ   ‚îú‚îÄ‚îÄchat_agent/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nodes.py   # Node functions\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py   # State definitions\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py       # Graph construction code\n‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ main.py                # LangGraph SDK demo\n‚îú‚îÄ‚îÄ .env                   # Environment variables (add OPENAI_API_KEY)\n‚îú‚îÄ‚îÄ .python-version        # Python version for this project (3.11)\n‚îî‚îÄ‚îÄ langgraph.json         # LangGraph API configuration\n\nInstall dependencies\nWe will be using uv for dependency management, if you haven‚Äôt used uv package manager before - now is the good time to start. You can install uv by following instructions on their official page.\nOnce installed run the below commands to set up the python environment.\nuv python install 3.11\nuv init\nOn successful completion of above commands, you will see pyproject.toml added to the project. This is uv equivalent for the requirements.txt for managing project dependencies and bundling the project.\nAppend following lines to pyproject.toml. This is so that uv recognizes the build system and can build and install your package into the project environment.\n\n\npyproject.toml\n\n[build-system]\nrequires = [\"setuptools&gt;=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\n\nFinally, install the dependencies.\nuv add \"langchain[openai]\" \"langgraph-cli[inmem]\" \\\n        langgraph langgraph-api langgraph-sdk"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#build",
    "href": "posts/langgraph-deployment/index.html#build",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Build",
    "text": "Build\nTime to get our hands dirty (in a good way)!\nWe will create a marketing research AI assistant using OpenAI‚Äôs gpt-4o model. Since we won‚Äôt be using any tools or any special patterns, our graph will be simple (intentionally). Let‚Äôs start adding code to the files we created.\n\nDefine the Agent State\nLangGraph uses a shared state object to pass information between nodes. Define the state using Python‚Äôs TypedDict to specify the data structure.\n\n\nstate.py\n\nfrom operator import add\nfrom typing import Annotated, TypedDict, Any\nfrom langchain_core.messages import AnyMessage\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]  # Accumulates messages\n\nThis state holds a list of messages (e.g., user input, AI responses) that are appended to as the conversation proceeds.\n\n\nImplement Node Functions\nNodes are the processing units of your graph. Each node is a function that takes the current state as input and returns an updated state. We will add a system message to instruct the model to follow a specific role, tone, or behavior during its execution within the node.\n\n\nnodes.py\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import SystemMessage\n\nfrom .state import State\n\n_ = load_dotenv()\n\nmodel = init_chat_model(model=\"gpt-4o\", model_provider=\"openai\")\n\ninstructions = (\n    \"You are a Marketing Research Assistant. \"\n    \"Your task is to analyze market trends, competitor strategies, \"\n    \"and customer insights. \"\n    \"Provide concise, data-backed summaries and actionable recommendations. \"\n)\nsystem_message = [SystemMessage(content=instructions)]\n\n\ndef chat(state: State) -&gt; dict:\n    messages = system_message + state[\"messages\"]\n    message = model.invoke(messages)  # Generate response\n    return {\"messages\": [message]}  # Return updated messages\n\n\n\nConstruct the Graph Workflow\nUse LangGraph‚Äôs StateGraph to build your agent‚Äôs workflow by adding nodes and edges that define the flow of execution.\n\n\nagent.py\n\nfrom langgraph.graph import END, START, StateGraph\nfrom utils.nodes import chat\nfrom utils.state import State\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chat\", chat)\n\ngraph_builder.add_edge(START, \"chat\")\ngraph_builder.add_edge(\"chat\", END)\n\ngraph = graph_builder.compile()"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#bundle",
    "href": "posts/langgraph-deployment/index.html#bundle",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Bundle",
    "text": "Bundle\nTypically, you‚Äôd need to write a backend to expose your application as an API. However, with LangGraph, that entire step is streamlined - simply add a config file and launch the service.\n\nLangGraph API Config\nTo configure the API, define the settings in langgraph.json by adding the following lines. The configuration is straightforward, so we won‚Äôt go into the details of each field here. However, if you‚Äôre interested in exploring advanced configuration options, you can refer to the official documentation here.\n\n\nlanggraph.json\n\n{\n    \"graphs\": {\n        \"agent\": \"chat_agent/agent.py:graph\"\n    },\n    \"env\": \".env\",\n    \"python_version\": \"3.11\",\n    \"dependencies\": [\n        \".\"\n    ]\n}\n\n\n\nLaunch Service\nOnce your configuration is in place, use the following command to bundle the application and launch it as a web service:\nlanggraph dev\nThis command not only generates an API with ready-to-use endpoints but also spins up a web-based chat interface (Studio UI) that makes it easy to test and debug your application in real time.\nüöÄ API: http://127.0.0.1:2024\nüé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\nüìö API Docs: http://127.0.0.1:2024/docs"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#deploy",
    "href": "posts/langgraph-deployment/index.html#deploy",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Deploy",
    "text": "Deploy\nAt this point, we have the LangGraph server running locally. Now, it‚Äôs time to deploy it. As shown in the component diagram, there are three primary deployment options available:\n\nCloud SaaS: Deploy LangGraph Servers to LangChain‚Äôs managed cloud infrastructure.\nSelf-hosted: Host and manage the LangGraph infrastructure within your own cloud environment - either just the data plane (with LangChain maintaining the control plane), or fully self-host both for complete control.\nStandalone container: Package the LangGraph Server as a Docker image and deploy it wherever you like - such as a Kubernetes cluster - while connecting to separately hosted Postgres and Redis instances.\n\nEach option comes with trade-offs:\n\nOption 1 requires the least setup but involves commercial licensing and vendor lock-in.\nOption 2 demands the most effort in terms of design, setup, and ongoing management, but offers maximum control.\nOption 3 strikes a balance, offering flexibility in deployment without full platform dependency.\n\nAlthough there are trade-offs either option can be selected based on your internal evaluation and convenience. We will explore the Option 3 - Standalone Container.\n\nStandalone Container\nLangGraph supports standalone container deployments using Docker, making it possible to run a scalable, self-managed service - ideal for deploying to environments like Kubernetes. This option is non-commercial and gives you full control over the infrastructure.\nWhile we won‚Äôt be deploying to a Kubernetes cluster in this guide, we‚Äôll simulate the setup locally using Docker to mirror the architecture shown in the diagram below. Specifically, we‚Äôll deploy three core services:\n\nLangGraph API ‚Äì the same service we configured and tested locally in the previous step.\nPostgreSQL ‚Äì used for persistent storage by LangGraph.\nRedis ‚Äì used for task orchestration and streaming output events through Pub/Sub.\n\nThis local deployment will serve as a lightweight, production-like replica, allowing us to validate the architecture and interactions between components without the overhead of a full Kubernetes setup.\n\n\n\n\n\n\nFigure¬†2: Standalone Container Deployment\n\n\n\n\n\nLocal Deployment with Docker Compose\nTo simplify service orchestration, we‚Äôll use Docker Compose to bring up the LangGraph API along with its required dependencies - PostgreSQL and Redis. This setup replicates a production-like environment locally and abstracts away the need to manage containers individually.\n\nLangGraph API\nIf the application is structured correctly, you can build a docker image with the LangGraph Deploy server.\nlanggraph build --tag chat_agent:v1.0.0\nThis command packages your application into a Docker image named chat_agent:v1.0.0, ready for deployment.\n\n\nDocker Compose\nWe‚Äôll define the services in a docker-compose.yml file as shown below.\n\n\ndocker-compose.yml\n\nvolumes:\n  langgraph-data:\n    driver: local\n\nservices:\n  langgraph-redis:\n    image: redis:6\n    healthcheck:\n      test: [ \"CMD\", \"redis-cli\", \"ping\" ]\n      interval: 5s\n      timeout: 1s\n      retries: 5\n\n  langgraph-postgres:\n    image: postgres:16\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_DB: postgres\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n    volumes:\n      - langgraph-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [ \"CMD\", \"pg_isready\", \"-U\", \"postgres\" ]\n      start_period: 10s\n      timeout: 1s\n      retries: 5\n      interval: 5s\n\n  langgraph-api:\n    image: ${IMAGE_NAME}\n    ports:\n      - \"8123:8000\"\n    depends_on:\n      langgraph-redis:\n        condition: service_healthy\n      langgraph-postgres:\n        condition: service_healthy\n    env_file:\n      - .env\n    environment:\n      REDIS_URI: redis://langgraph-redis:6379\n      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}\n      POSTGRES_URI: postgres://postgres:postgres@langgraph-postgres:5432/postgres?sslmode=disable\n\n\n\nRunning the Application\nEnsure that your .env file includes valid values for IMAGE_NAME and LANGSMITH_API_KEY before running the Docker Compose setup.\n\n\n.env\n\nOPENAI_API_KEY=your-openai-api-key\nLANGSMITH_API_KEY=your-langsmith-api-key\nIMAGE_NAME=chat_agent:v1.0.0\n\nOnce everything is set, start the stack by running:\ndocker compose up --build\nThis will -\n\nStart and link the required Redis and PostgreSQL containers\nBuild and launch your LangGraph API container\nWait for dependent services to become healthy before launching the API\nExpose the API locally at http://localhost:8123\n\nYou can test that the application is up by checking:\ncurl --request GET --url 0.0.0.0:8123/ok\nAssuming everything is running correctly, you should see a response like:\n{\"ok\":true}"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#interact",
    "href": "posts/langgraph-deployment/index.html#interact",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Interact",
    "text": "Interact\nIf you‚Äôve made it this far - congratulations! üéâ\nThe final step is to interact with your deployed LangGraph server using the LangGraph SDK.\nLangGraph provides SDKs for both Python and JavaScript/TypeScript, making it easy to work with deployed graphs - whether hosted locally or in the cloud. The SDK abstracts away the complexity of direct API calls and provides:\n\nA unified interface to connect with LangGraph graphs\nEasy access to assistants and conversation threads\nSimple run execution and real-time streaming capabilities\n\nFollowing is the code-snippet in Python to demonstrate interaction with the graph we deployed in the previous step -\n\n\nmain.py\n\nimport asyncio\n\nfrom langchain_core.messages import HumanMessage\nfrom langgraph_sdk import get_client\n\nLANGGRAPH_SERVER_URL = \"http://localhost:8123\"\n\nasync def main() -&gt; None:\n\n    # Initialize the LangGraph client\n    client = get_client(url=LANGGRAPH_SERVER_URL)\n\n    # Fetch the list of available assistants from the server\n    assistants = await client.assistants.search()\n\n    # Select the first assistant from the list\n    agent = assistants[0]\n\n    # Create a new conversation thread with the assistant\n    thread = await client.threads.create()\n\n    # Prepare the user message\n    user_message = {\"messages\": [HumanMessage(content=\"Hi\")]}\n\n    # Stream the assistant's response in real-time\n    async for chunk in client.runs.stream(\n        thread_id=thread[\"thread_id\"],\n        assistant_id=agent[\"assistant_id\"],\n        input=user_message,\n        stream_mode=\"values\",\n    ):\n        # Filter out metadata events and empty data chunks\n        if chunk.data and chunk.event != \"metadata\":\n            # Print the content of latest assistant message\n            print(chunk.data[\"messages\"][-1][\"content\"])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nThe above code is intuitive, so I‚Äôll leave it to you to run and explore.\nAt first glance, the response you get might feel modest - a simple message in return for a simple prompt. But behind that is a clean, scalable setup that mirrors how production-grade AI systems are built. You‚Äôve laid the groundwork for something much bigger. Swapping out this basic agent for a more advanced one is just a matter of choice now. So while it might not feel flashy, make no mistake - you‚Äôve just put a serious system in place.\n\nWant to take this further? Try integrating it into a Streamlit frontend to create an interactive, real-time chat experience."
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#further-reading",
    "href": "posts/langgraph-deployment/index.html#further-reading",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Further Reading",
    "text": "Further Reading\n\nHow to Deploy to Cloud SaaS\nSelf-Hosted Data Plane\nSelf-Hosted Control Plane\nHow to add custom authentication\nHow to add custom routes\nHow to integrate LangGraph into your React application\nLangGraph SDK"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#references",
    "href": "posts/langgraph-deployment/index.html#references",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "References",
    "text": "References\n\nLangGraph Deployment Options\nHow to do a Self-hosted deployment of LangGraph\nHow to Deploy a Standalone Container"
  },
  {
    "objectID": "posts/context-engineering-part-1/index.html",
    "href": "posts/context-engineering-part-1/index.html",
    "title": "Context Engineering: Building Smarter AI Agents - Part 1/3",
    "section": "",
    "text": "For the past two years, I‚Äôve been building generative AI applications for businesses - everything from straightforward workflows that automate simple tasks to sophisticated AI agents powering complex conversational experiences. Lately, I‚Äôve noticed the term ‚Äúcontext engineering‚Äù popping up everywhere in the AI community. I dug into what it actually means, only to realise it‚Äôs essentially what I‚Äôve already been doing while developing agentic AI systems with frameworks like LangGraph. It turns out, the practices behind context engineering aren‚Äôt new - they‚Äôve just finally been given a name.\nIn this post - the first in a three-part series - I‚Äôll share a deep dive into context engineering, drawing on my experience building agentic AI applications for real-world businesses. I‚Äôll illustrate key concepts with practical examples and walk through LangGraph code snippets to show how it naturally supports context engineering in practice. So, sit back, relax, and enjoy the read! üìñ"
  },
  {
    "objectID": "posts/context-engineering-part-1/index.html#beyond-prompt-engineering-the-rise-of-context-engineering",
    "href": "posts/context-engineering-part-1/index.html#beyond-prompt-engineering-the-rise-of-context-engineering",
    "title": "Context Engineering: Building Smarter AI Agents - Part 1/3",
    "section": "Beyond Prompt Engineering ‚Äì The Rise of Context Engineering",
    "text": "Beyond Prompt Engineering ‚Äì The Rise of Context Engineering\n\nThe Evolution of LLM Interaction\nIn the rapidly evolving landscape of Artificial Intelligence, Large Language Models (LLMs) have transitioned from being mere conversational tools to becoming the core intelligence of sophisticated, dynamic agentic systems. Initially, interacting with LLMs primarily involved crafting precise, single-turn prompts ‚Äì a practice widely known as prompt engineering. This approach, while effective for specific, isolated queries, quickly revealed its limitations when faced with complex, multi-step tasks or ongoing dialogues. The need for LLMs to maintain state, interact with external tools, and engage in multi-turn reasoning led to the development of more advanced interaction paradigms.\n\n\nWhat is Context Engineering?\nAs LLM applications grew in complexity, a new discipline emerged: Context Engineering. At its heart, context engineering is the art and science of meticulously managing the information an LLM uses to think, act, and decide. It‚Äôs about constructing dynamic systems that provide the right information and tools, in the optimal format, at precisely the right moment, to enable an LLM to accomplish a given task reliably and effectively.\nTo distinguish it from prompt engineering: while prompt engineering focuses on the user-facing input to produce a desired response, context engineering is a developer-facing discipline. It delves into the underlying architecture and data flows, ensuring that the LLM receives a comprehensive and well-structured input that goes far beyond a simple prompt. This comprehensive input, or ‚Äòcontext,‚Äô encompasses a wide array of elements, including:\n\nSystem Instructions: High-level directives that define the LLM‚Äôs role, persona, and constraints.\nRetrieved Content: Information pulled from external knowledge bases, databases, or documents relevant to the current task.\nTool Outputs: Results from external tools or APIs that the LLM has invoked (e.g., search results, database queries, code execution outputs).\nConversation History: The ongoing dialogue, summarised or filtered to maintain coherence and continuity.\nExternal Data: Any other pertinent data points, such as user preferences, environmental variables, or real-time sensor data.\n\nIn essence, context engineering acknowledges that the quality of an LLM‚Äôs output is directly proportional to the quality and relevance of the context it receives. It‚Äôs about building the intelligent scaffolding around the LLM that allows it to perform at its peak.\n\n\nWhy is Context Engineering Crucial?\nThe adage ‚Äúgarbage in, garbage out‚Äù holds particularly true for LLMs. Without relevant, precise, and well-organised context, even the most advanced LLMs can produce unreliable, irrelevant, or hallucinated outputs. Most failures observed in complex AI agents are not due to the inherent limitations of the LLM itself, but rather to what we term ‚Äúcontext failures‚Äù ‚Äì instances where the model was not provided with the necessary information to make an informed decision or generate an accurate response.\nThis is where LangGraph emerges as a pivotal framework. LangGraph, a powerful library within the LangChain ecosystem, provides developers with granular control over the flow of information and the management of context within LLM applications. By enabling the explicit definition of stateful, cyclic workflows, LangGraph allows for meticulous context engineering, ensuring that the LLM always operates with the most pertinent and up-to-date information. It transforms the abstract concept of context management into a tangible, programmable reality, paving the way for the development of truly intelligent and reliable AI agents."
  },
  {
    "objectID": "posts/context-engineering-part-1/index.html#pillars-of-context-in-llm-applications",
    "href": "posts/context-engineering-part-1/index.html#pillars-of-context-in-llm-applications",
    "title": "Context Engineering: Building Smarter AI Agents - Part 1/3",
    "section": "Pillars of Context in LLM Applications",
    "text": "Pillars of Context in LLM Applications\nTo effectively engineer context, it‚Äôs crucial to understand its multifaceted nature within LLM applications. Context is not a static entity; it‚Äôs a dynamic and evolving construct that underpins the LLM‚Äôs ability to reason and act.\n\nDynamic and Evolving Context\nUnlike traditional software where inputs are often fixed at the beginning of a process, the context for an LLM in an agentic system is assembled and continuously updated on the fly. As a task progresses, new information becomes available, previous actions yield results, and the LLM‚Äôs internal state evolves. This necessitates a system that can dynamically manage and update the context provided to the model.\n\nConsider a customer support chatbot. The initial context might be the user‚Äôs first query. As the conversation unfolds, the context expands to include the entire conversation history, the chatbot‚Äôs previous responses, and any information retrieved from a knowledge base or CRM system. If the user asks for an order status, the system needs to dynamically add order details to the context before querying the LLM.\n\n\n\nFull Contextual Coverage\nIt‚Äôs not enough to provide just the immediate query; for an LLM to perform optimally, it requires full contextual coverage. This means supplying all necessary information that might influence its understanding, reasoning, or generation. This includes not only the explicit user input but also implicit background information, system-level instructions, and relevant external data.\n\nFor a code generation agent, full contextual coverage would involve not just the user‚Äôs request for a function, but also the project‚Äôs coding standards, existing library dependencies, relevant API documentation, and even examples of similar functions within the codebase. Omitting any of these could lead to suboptimal or incorrect code.\n\n\n\nTools and External Information\nOne of the most significant advancements in LLM applications is their ability to leverage external tools and access real-time information. This capability is a cornerstone of effective context engineering, as it allows LLMs to overcome their inherent knowledge limitations (i.e., being trained on a fixed dataset) and interact with the dynamic real world. Tools can range from simple search engines and calculators to complex APIs for database interaction, code execution, or external service calls.\n\nAn LLM agent tasked with planning a trip needs access to real-time flight prices, hotel availability, and weather forecasts. These pieces of information are not part of its pre-trained knowledge; they must be retrieved dynamically through external tools (e.g., flight booking APIs, weather APIs) and then integrated into the context for the LLM to process and generate a coherent travel plan.\n\n\n\nMemory Management (Short-term & Long-term)\nEffective context engineering relies heavily on robust memory management, which can be broadly categorised into short-term and long-term memory:\n\nShort-term Memory: This refers to the immediate, transient context of an ongoing interaction. For LLMs, this often involves managing the conversation history within the model‚Äôs context window. Since context windows have limits, strategies like summarisation, truncation, or selective retention are crucial to preserve the most relevant parts of the dialogue.\n\n\nIn a multi-turn conversation, a chatbot might summarise the previous 10 turns into a concise paragraph to keep the LLM informed about the conversation‚Äôs trajectory without exceeding the context window limit.\n\n\nLong-term Memory: This involves persisting and retrieving information that spans across multiple sessions or is relevant to a user‚Äôs enduring preferences or past interactions. This often utilises external databases, vector stores, or knowledge graphs.\n\n\nA personalised shopping assistant might store a user‚Äôs past purchases, preferred brands, and sizing information in a long-term memory system. When the user returns, this information can be retrieved and added to the context, allowing the LLM to offer highly relevant product recommendations.\n\n\n\nStructured Data and Format\nThe way information is presented to an LLM significantly impacts its ability to process and utilise that information. Well-organised, structured data (e.g., JSON, XML, or clearly delimited text) is far more effective than unstructured, messy data. Providing context in a consistent and predictable format reduces ambiguity and improves the LLM‚Äôs parsing and reasoning capabilities.\n\nWhen providing search results to an LLM, presenting them as a list of JSON objects with clear keys (e.g., {\"title\": \"...\", \"url\": \"...\", \"snippet\": \"...\"}) is much more effective than simply concatenating raw text. The structured format allows the LLM to easily identify and extract specific pieces of information, leading to more accurate and relevant responses."
  },
  {
    "objectID": "posts/context-engineering-part-1/index.html#conclusion",
    "href": "posts/context-engineering-part-1/index.html#conclusion",
    "title": "Context Engineering: Building Smarter AI Agents - Part 1/3",
    "section": "Conclusion",
    "text": "Conclusion\nIn this first part of our three-part series, we‚Äôve unpacked the foundations of context engineering - what it is, why it matters, and how it underpins the next generation of agentic AI systems. By exploring the pillars of context, from dynamic memory management to the integration of external tools and structured data, we‚Äôve set the stage for building truly intelligent, reliable LLM applications.\nIn Part 2, we‚Äôll dive into how LangGraph provides the architectural framework to put these principles into practice, enabling you to engineer context with precision and flexibility. Stay tuned!"
  },
  {
    "objectID": "posts/langgraph-mcp/index.html",
    "href": "posts/langgraph-mcp/index.html",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "",
    "text": "If you‚Äôve been following AI developments lately, you‚Äôve likely seen the term¬†MCP¬†(Model Control Protocol) gaining traction. But what exactly is it, and why does it matter for the future of AI?\nIn simple terms, MCP is a¬†standardised way for AI models to interact with external tools and services, unlocking capabilities beyond just generating text. To understand why this is a big deal, let‚Äôs break down the problem it solves."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#the-limitation-ai-without-tools-is-like-a-brain-without-hands",
    "href": "posts/langgraph-mcp/index.html#the-limitation-ai-without-tools-is-like-a-brain-without-hands",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "The Limitation: AI Without Tools Is Like a Brain Without Hands",
    "text": "The Limitation: AI Without Tools Is Like a Brain Without Hands\nAt their core, large language models (LLMs) are incredibly skilled at¬†predicting and generating text. Ask one to summarise an article or draft a story, and it performs impressively. But ask it to¬†do¬†something - like checking the weather, updating a calendar, or retrieving live data - and it falls short.\nWhy? Because¬†LLMs alone can‚Äôt interact with the outside world. They‚Äôre like a brilliant mind trapped in a room with no doors - knowledgeable, but unable to act."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#the-first-solution-connecting-ai-to-tools",
    "href": "posts/langgraph-mcp/index.html#the-first-solution-connecting-ai-to-tools",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "The First Solution: Connecting AI to Tools",
    "text": "The First Solution: Connecting AI to Tools\nTo make AI truly useful, developers began linking LLMs to¬†external tools¬†(APIs, databases, web services, etc.). For example:\n\nA chatbot could fetch real-time stock prices by connecting to a financial API.\nAn AI assistant could schedule meetings by integrating with a calendar service.\n\nBut there‚Äôs a catch:¬†Every tool operates differently.\n\nA weather API expects inputs in a specific format.\nA database might require unique authentication steps.\nA task manager could have its own rules for creating entries.\n\nThis means developers have to¬†manually customise¬†each connection - a tedious and fragile process. If one service updates its API, the entire integration can break."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#mcp-the-universal-plug-play-system-for-ai-tools",
    "href": "posts/langgraph-mcp/index.html#mcp-the-universal-plug-play-system-for-ai-tools",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "MCP: The Universal Plug & Play System for AI Tools",
    "text": "MCP: The Universal Plug & Play System for AI Tools\n\n\n\n\n\n\nFigure¬†1: MCP Conceptual Architecture (Sakal 2024)\n\n\n\nToday‚Äôs AI assistants struggle because every tool connection requires custom wiring. MCP changes this by becoming the¬†USB-like standard¬†for AI - where any compatible service can plug in and work immediately, just like devices with your MacBook using a USB-C hub a.k.a docking station (as illustrated in the conceptual arcitecture).\n\nHow MCP Actually Works\nThe definition of MCP as per the offical documentation is as follows -\n\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\n\nThe MCP Server - ‚ÄúUSB Devices‚Äù\n\nEach tool or service - Slack, Gmail, Calendar, local files - runs an MCP server\nThese are like USB devices: they translate their native APIs into the standard MCP format\nExample: When the Calendar server gets a request like ‚Äúlist events‚Äù, it maps that to Google Calendar‚Äôs internal API and returns it in a universal format\n\n\n\nThe MCP Client - ‚ÄúAgentic AI Application‚Äù\n\nLives inside AI agents like Claude, Cursor, etc.\nIt‚Äôs like your MacBook: it doesn‚Äôt care what‚Äôs plugged in - as long as it follows the USB standard (the MCP protocol), it can use it immediately\nExample: When you say ‚ÄúSend an email‚Äù, the MCP client routes the request through the hub to the Gmail server, without needing to know Gmail‚Äôs API details\n\n\n\nThe MCP Protocol - ‚ÄúUSB Hub‚Äù\nThe protocol is the hub connecting the client (AI) and servers (tools). It defines the standard interface they all speak:\n\nUniform request formats (get_email, list_files, etc.)\nConsistent response schemas (same field names, types)\nPredictable error handling\n\nOnce a tool implements this protocol, any AI can access it instantly - just like plugging into a USB hub.\n\n\n\nWhy This Isn‚Äôt Just ‚ÄúAnother API‚Äù\n\n\n\n\n\n\n\nTraditional Approach\nMCP Approach\n\n\n\n\nEach tool needs custom code\nTools work immediately when installed\n\n\nAPIs break when updated\nBuilt-in version control prevents breaks\n\n\nAI must learn each tool‚Äôs quirks\nStandardized interactions eliminate guesswork\n\n\nHard to combine tools\nTools can ‚Äúautomagically‚Äù work together\n\n\n\n\nReal-World Impact\nAn MCP-enabled AI could:\n\nNotice your calendar shows an outdoor meeting\n\nCheck the weather MCP server for rain forecasts\nCross-reference traffic MCP server for delays\nPropose rescheduling -¬†all without pre-programmed rules\n\nThis is why developers are excited: MCP isn‚Äôt just improving tools - it‚Äôs creating an ecosystem where AI can truly¬†understand¬†and¬†use¬†services as flexibly as humans do."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#why-this-matters-for-ais-future",
    "href": "posts/langgraph-mcp/index.html#why-this-matters-for-ais-future",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "Why This Matters for AI‚Äôs Future",
    "text": "Why This Matters for AI‚Äôs Future\nToday‚Äôs AI assistants are limited because each tool integration requires manual, brittle connections - like a smart home where every device speaks a different language and needs custom programming (no longer the case with introduction of standard Matter protocol, MCP equivalent for smart homes). MCP changes this by introducing a universal standard, enabling three key breakthroughs:\n\nDynamic Tool Discovery - Instead of hard-coding every API, AI systems can automatically discover and use new MCP-compatible tools, much like plug-and-play USB devices.\n\nSelf-Healing Connections - Unlike current systems that break when APIs update, MCP builds in versioning and fallback methods, keeping workflows intact.\n\nMulti-Tool Reasoning - Today, chaining actions across services (e.g., checking traffic, rescheduling meetings, and notifying teams) requires months of custom development. With MCP, AI can dynamically combine any compliant tools on the fly, enabling complex, cross-platform automation without pre-built pipelines.\n\nThis shift turns AI from a tool that merely responds into a system that orchestrates - seamlessly blending services the way humans intuitively combine tools to solve problems. The result? Assistants that don‚Äôt just follow instructions but proactively adapt to real-world complexity."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#mcp-in-practice-building-an-ai-agent-with-model-control-protocol",
    "href": "posts/langgraph-mcp/index.html#mcp-in-practice-building-an-ai-agent-with-model-control-protocol",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "MCP in Practice: Building an AI Agent with Model Control Protocol",
    "text": "MCP in Practice: Building an AI Agent with Model Control Protocol\nNow that we understand MCP‚Äôs conceptual framework, let‚Äôs examine how it works in practice through a concrete implementation. We‚Äôll explore a simple math-solving AI agent that connects to MCP-enabled tools.\n\nAnatomy of an MCP System\nOur example consists of two core components:\n\nMCP Server: The ‚Äútool provider‚Äù that exposes mathematical operations\nMCP Client: The AI agent that leverages these tools dynamically\n\n\nThe MCP Server: Math as a Service ü™ø\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp_server.tools import math_tools\n\nmcp = FastMCP(\"MCP Server\")\n\n# Register tools with MCP instance using decorators dynamically\nmcp.tool()(math_tools.add)\nmcp.tool()(math_tools.multiply)\n\nif __name__ == \"__main__\":\n    try:\n        mcp.run(transport=\"streamable-http\")\n        print(\"Started MCP server\")\n    except KeyboardInterrupt:\n        print(\"Exiting...\")\n    except Exception as e:\n        print(f\"Failed to start MCP Server - {e}\")\n\nThe code uses official mcp python-sdk to implement a MCP Server. The repo comes with more examples in README.\n\nKey features worth noting:\n\nDeclarative Tool Registration: Tools are added using simple decorators (@mcp.tool())\nTransport Agnostic: The server can use different communication protocols (here using HTTP streaming)\nModular Design: Tools are organized in separate modules (tools/math_tools.py), enabling clean separation of concerns and clean extensibility.\n\nThe server exposes two basic operations - addition and multiplication - but could easily scale to include hundreds of tools with the same lightweight pattern.\n\n\nThe MCP Client: AI That ‚ÄúJust Knows‚Äù How to Use Tools üß†\nimport asyncio\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage\n\nasync def main():\n    client = MultiServerMCPClient({\n        \"math\": {\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transport\": \"streamable_http\",\n        }\n    })\n\n    tools = await client.get_tools()\n    agent = create_react_agent(\"openai:gpt-4.1\", tools)\n\n    response = await agent.ainvoke({\n        \"messages\": [HumanMessage(content=\"what's (3 + 5) * 12?\")]\n    })\n\n    response[\"messages\"][-1].pretty_print()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nWe are using LangGraph to demonstrate client, but it would work with any Agent framework.\n\nKey features worth noting:\n\nAutomatic Tool Discovery: The client dynamically fetches available tools from the server\nZero Tool-Specific Code: The AI understands how to use add and multiply without explicit programming\nNatural Language Interface: The operation (3 + 5) * 12 is solved through conversational interaction\n\nWhen executed, this agent will:\n\nParse the user‚Äôs math question\nDetermine it needs to first add 3 and 5\nThen multiply the result by 12\nReturn the correct answer (96) - all by dynamically composing the available MCP tools"
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#from-math-to-real-world-applications",
    "href": "posts/langgraph-mcp/index.html#from-math-to-real-world-applications",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "From Math to Real-World Applications",
    "text": "From Math to Real-World Applications\nWhile our example uses simple math operations, the same pattern scales to enterprise use cases:\n# Hypothetical enterprise MCP server\nmcp.tool()(salesforce.get_opportunities)\nmcp.tool()(jira.create_ticket)\nmcp.tool()(slack.send_message)\nmcp.tool()(bigquery.run_query)\nAn AI agent with access to these tools could:\n\nQuery Salesforce for new deals\nCreate Jira tickets for follow-ups\nNotify teams via Slack\nLog actions in BigQuery\n\nAll through natural language requests, with no pre-built workflows."
  },
  {
    "objectID": "posts/langgraph-mcp/index.html#do-i-need-mcp-server-for-my-project",
    "href": "posts/langgraph-mcp/index.html#do-i-need-mcp-server-for-my-project",
    "title": "MCP Explained: The Bridge Between AI and the Real World",
    "section": "Do I need MCP Server for my Project?",
    "text": "Do I need MCP Server for my Project?\n‚≠ï Are you building an Agentic AI Application with tools?\n‚≠ï Is any tool calling an internal/external API?\n‚≠ï Do you manage multiple, ever-changing APIs?\n‚≠ï Should your AI dynamically combine tools?\n‚≠ï Will you add more tools over time?\nIf you answered YES to 2 or more questions, MCP will save you time, reduce fragility, and future-proof your AI stack.\nNext Steps\n\nStart small: Pick one high-impact tool and MCP-enable it.\nUse the checklist above to justify MCP adoption to your team.\nMonitor ROI: Track reduced dev hours and increased AI capabilities post-MCP."
  },
  {
    "objectID": "posts/langgraph-redis/index.html",
    "href": "posts/langgraph-redis/index.html",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "",
    "text": "In How to Build and Deploy an AI Agent using LangGraph from Scratch, we deployed an AI agent to a standalone container running the LangGraph server. This setup communicates with both PostgreSQL and Redis. In that architecture, PostgreSQL serves as a checkpointer - responsible for persisting agent state across interactions, allowing the agent to ‚Äúremember‚Äù previous conversations.\nHowever, the role of Redis may have felt less clear. It‚Äôs briefly described as a task orchestrator and a channel for streaming output, but beyond that, LangGraph‚Äôs documentation doesn‚Äôt dive deep into its purpose or inner workings.\nOne of the advantages of using open-source software is transparency - we can inspect the implementation to truly understand what‚Äôs going on under the hood. So that‚Äôs exactly what I did. 1\nIn this article, we‚Äôll explore how Redis fits into the LangGraph architecture and how it plays a crucial role in enabling fault-tolerant, stateful conversational AI applications, especially when working alongside PostgreSQL and the LangGraph server."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#redis-in-langgraph",
    "href": "posts/langgraph-redis/index.html#redis-in-langgraph",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Redis in LangGraph",
    "text": "Redis in LangGraph\nAt its core, Redis serves as LangGraph‚Äôs high-performance messaging backbone, enabling two critical capabilities:\n\nTask queuing with retry counts for fault-tolerant agent workflows\nReal-time streaming of intermediate agent outputs\n\nUnlike traditional databases, Redis specializes in low-latency operations through its in-memory data structures. For LangGraph, this means:\n\nRedis List act as FIFO queues for agent task scheduling\nRedis String and Pub/Sub for bi-directional signaling (output streaming / cancellations)\nBlocking Queues create reliable queues that survive worker crashes\n\nThese concepts will be clear once we start looking into the internal working.\n\n\n\n\n\n\nNote\n\n\n\nBy using Redis as a message broker, LangGraph decouples the components that produce runs (agents/servers) from the workers that execute them.\n\n\n\nDuality\nA key architectural advantage is Redis‚Äô ability to function as both a queue and a broadcast system simultaneously. When your LangGraph agent processes a request:\n\nPostgreSQL persists the final state (the ‚Äúwhat‚Äù)\nRedis orchestrates the execution path (the ‚Äúhow‚Äù)\n\nThis explains why LangGraph requires both databases - while PostgreSQL provides durability, Redis delivers the coordination layer that makes stateful, long-running agent workflows possible."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#under-the-hood",
    "href": "posts/langgraph-redis/index.html#under-the-hood",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Under the hood",
    "text": "Under the hood\nThe following sequence diagram captures the key interactions and the role of Redis as a messaging and signaling layer, with Postgres as the durable run data store.\n\n\n\n\n\n\nFigure¬†1: How LangGraph uses Redis\n\n\n\n\nExplanation\nThe Agent creates a new run by inserting it into Postgres and signals workers2 via a sentinel3 pushed to a Redis list.\n\nThe Worker blocks on Redis list (BLPOP4) waiting for a wake-up signal.\nUpon receiving the signal, the worker fetches the actual run data from Postgres.\nThe worker executes the run asynchronously.\nDuring execution, the worker streams output events via Redis PubSub to the agent.\nIf a cancellation is requested, the agent sets a cancellation flag in Redis, which is communicated to the worker via PubSub channel.\nUpon completion, the worker updates the run status in Postgres, clears ephemeral metadata in Redis, and notifies the agent (e.g., via webhook)."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#crash-resilience",
    "href": "posts/langgraph-redis/index.html#crash-resilience",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Crash Resilience",
    "text": "Crash Resilience\nAt the heart of LangGraph‚Äôs fault-tolerance lies its use of Redis lists as a transactional queue. This is achieved via BLPOP and Atomic Task Handoffs.\n\nCrash-Safe Task Claiming\nWhen a worker executes a BLPOP tasks: queue 0 (as shown in the sequence diagram), the operation is atomic5:\n\nRedis only removes the task from the list after it has been delivered to the worker.\nIf the worker crashes before processing the task, the task remains in the queue.\n\nThis ensures that no task is lost due to unexpected crashes or restarts. Additionally, LangGraph sets a sensible default for concurrency control:\nN_JOBS_PER_WORKER = env(\"N_JOBS_PER_WORKER\", cast=int, default=10)\nEach worker will process up to 10 tasks in parallel unless configured otherwise, helping scale horizontally while keeping workloads isolated.\n\n\nThe Sentinel Pattern\nIn the diagram, you may have noticed agents pushing a sentinel - a wake-up signal - into Redis.\n\n\n\n\n\n\nFigure¬†2: The Sentinel Pattern\n\n\n\nHere‚Äôs how it works:\n\nWorkers use BLPOP with a timeout of 0, meaning they block indefinitely until a signal arrives.\nThis design eliminates the need for polling and ensures no messages are missed - even if a worker restarts.\n\nLangGraph also reports internal metrics periodically to help with observability:\nSTATS_INTERVAL_SECS = env(\"STATS_INTERVAL_SECS\", cast=int, default=60)\nThis allows systems to track worker health and performance at one-minute intervals by default.\n\n\nPostgreSQL as a Fallback\nIf Redis is temporarily unavailable or restarts, LangGraph falls back to PostgreSQL via Runs.next, as shown in the query step of the diagram.\nThis creates a two-layered recovery mechanism:\n\nRedis-first (hot path): Fast, in-memory task delivery.\nPostgreSQL (cold path): Durable, persistent task storage.\n\nThis layered approach ensures resilience without sacrificing performance. LangGraph also sets an upper bound on how long a background task can run:\nBG_JOB_TIMEOUT_SECS = env(\"BG_JOB_TIMEOUT_SECS\", cast=float, default=3600)\nThis ensures that long-running or stuck jobs don‚Äôt hang forever - by default, any job exceeding 1 hour is forcefully timed out.\n\n\nWhy This Beats Polling\nPolling-based systems often suffer from subtle timing issues. For instance, if a worker uses RPOP6 to dequeue a task and then crashes before it begins processing, that task is effectively lost. Similarly, network hiccups or buffering delays can cause tasks to be silently dropped or missed altogether.\nLangGraph avoids these pitfalls by using BLPOP, which blocks server-side until a task is available. This means tasks remain in Redis until they are actually handed off to a live worker. There are no polling loops, no race conditions, and no dependency on fragile network timing - just clean, atomic task delivery."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#putting-it-all-together",
    "href": "posts/langgraph-redis/index.html#putting-it-all-together",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Putting it All Together",
    "text": "Putting it All Together\nIn summary, LangGraph cleverly integrates Redis not just as a simple cache, but as a core component for resilient task orchestration and real-time communication. By leveraging atomic operations like BLPOP for crash-safe task claiming and Pub/Sub for efficient signaling and output streaming, it builds a robust system that complements PostgreSQL‚Äôs role in state persistence. This dual-database approach allows LangGraph to deliver performant, fault-tolerant execution for complex, stateful AI agent workflows, ensuring tasks are reliably processed even amidst potential failures."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#references",
    "href": "posts/langgraph-redis/index.html#references",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "References",
    "text": "References\n\nRedis Queue\nLangGraph: How we use Redis"
  },
  {
    "objectID": "posts/langgraph-redis/index.html#footnotes",
    "href": "posts/langgraph-redis/index.html#footnotes",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLangGraph doesn‚Äôt publish source code for langgraph-storage, but only wheels. So you have to install the package to view the code. The implementation is present in langgraph_storage.queue‚Ü©Ô∏é\nWorkers in this context are async tasks in Python, they have nothing to do with Redis.‚Ü©Ô∏é\nA sentinel is a special placeholder or signal value used within a Redis list to notify workers - it has no actual run info. Not to be confused with Redis Sentinel (the high-availability system).‚Ü©Ô∏é\nIn a basic queue, if a consumer tries to dequeue a task when the queue is empty, it gets a null response and may need to poll the queue repeatedly. To avoid this, Redis provides a way to implement blocking queues. In a blocking queue, if a consumer tries to dequeue a task when the queue is empty, it is put to sleep by Redis until a task is available.‚Ü©Ô∏é\nAtomic tasks means, they either complete fully or not at all‚Ü©Ô∏é\nRPOP: Redis command to dequeue an element (FIFO)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/context-engineering-part-3/index.html",
    "href": "posts/context-engineering-part-3/index.html",
    "title": "Context Engineering: Building Smarter AI Agents - Part 3/3",
    "section": "",
    "text": "In Part 1 and Part 2 of this series, we introduced Context Engineering and explored how to use LangGraph to build context-aware AI applications, however, building robust and intelligent LLM applications with LangGraph requires more than just understanding the syntax; it demands a strategic approach to context engineering. Here are some best practices to guide your development:\n\n\nBefore you even start coding, clearly articulate what you want your LLM application to achieve. What are its primary functions? What are its limitations? Understanding the scope and purpose will help you identify what context is truly necessary and what is superfluous. For instance, a customer service bot needs different context than a creative writing assistant.\n\n\n\nThe quality of your LLM‚Äôs output is directly proportional to the quality and relevance of the context it receives.\n\nRelevance is Key: Only provide information that is directly pertinent to the current task. Overloading the context window with irrelevant data can lead to confusion, increased token usage, and diminished performance.\nPrecision Matters: Ensure the information is accurate and unambiguous. Vague or contradictory context will lead to vague or contradictory responses.\nStructure for Clarity: Present context in a clear, consistent, and structured format. Use JSON, XML, or well-defined Markdown to organise information. This makes it easier for the LLM to parse and extract the necessary details.\nPrioritise Information: If context window limits are a concern, prioritise the most critical information. Techniques like summarisation or hierarchical retrieval can help manage large volumes of data.\n\n\n\n\nEffective memory management is fundamental to maintaining coherent and personalised interactions.\n\nShort-term Memory (Conversation History): For ongoing dialogues, implement strategies to manage conversation history. This might involve:\n\nSummarisation: Periodically summarising past turns to condense the history while retaining key information.\nWindowing: Keeping only the most recent N turns or a fixed number of tokens.\nHybrid Approaches: Combining summarisation with a sliding window.\n\nLong-term Memory (Knowledge Bases): For information that needs to persist across sessions or is too large for the context window, integrate with external knowledge bases. This typically involves:\n\nVector Databases: Storing embeddings of documents and retrieving relevant chunks based on semantic similarity (as seen in the RAG example).\nTraditional Databases: Storing structured data like user profiles, preferences, or past transactions.\nKnowledge Graphs: Representing complex relationships between entities for more sophisticated retrieval.\n\n\n\n\n\nTools extend the capabilities of your LLM, allowing it to interact with the real world and access up-to-date information.\n\nDefine Tool Capabilities Clearly: Ensure your tools have clear, concise descriptions that accurately convey their function to the LLM. The LLM relies on these descriptions to decide when and how to use a tool.\nHandle Tool Outputs: Design your graph to effectively process and integrate tool outputs back into the context. This might involve parsing JSON responses, summarising long text outputs, or handling errors gracefully.\nConditional Tool Use: Leverage LangGraph‚Äôs conditional edges to enable the LLM to dynamically decide whether a tool call is necessary based on the current query and context. This prevents unnecessary tool invocations and improves efficiency.\n\n\n\n\nContext engineering is rarely a one-shot process. It requires continuous refinement.\n\nStart Simple: Begin with a basic graph and gradually add complexity. Test each component and connection thoroughly.\nTest Edge Cases: Don‚Äôt just test happy paths. Actively try to break your system by providing ambiguous queries, unexpected inputs, or requests that require complex reasoning or tool use.\nMonitor Performance: Track metrics like response time, accuracy, and token usage. Identify bottlenecks or areas where context management can be improved.\n\n\n\n\nTools like LangSmith (from the creators of LangChain and LangGraph) are invaluable for debugging and understanding your LLM applications.\n\nTrace Agent Calls: LangSmith allows you to visualise the execution flow of your LangGraph application, showing you exactly what context was passed to each node, what the LLM‚Äôs thought process was, and what tool calls were made. This is critical for identifying where context might be failing or where the LLM is misinterpreting information.\nEvaluate Context Effectiveness: Use these traces to evaluate whether the context you are providing is indeed leading to the desired outcomes. Adjust your context engineering strategies based on these insights."
  },
  {
    "objectID": "posts/context-engineering-part-3/index.html#best-practices-for-context-engineering-with-langgraph",
    "href": "posts/context-engineering-part-3/index.html#best-practices-for-context-engineering-with-langgraph",
    "title": "Context Engineering: Building Smarter AI Agents - Part 3/3",
    "section": "",
    "text": "In Part 1 and Part 2 of this series, we introduced Context Engineering and explored how to use LangGraph to build context-aware AI applications, however, building robust and intelligent LLM applications with LangGraph requires more than just understanding the syntax; it demands a strategic approach to context engineering. Here are some best practices to guide your development:\n\n\nBefore you even start coding, clearly articulate what you want your LLM application to achieve. What are its primary functions? What are its limitations? Understanding the scope and purpose will help you identify what context is truly necessary and what is superfluous. For instance, a customer service bot needs different context than a creative writing assistant.\n\n\n\nThe quality of your LLM‚Äôs output is directly proportional to the quality and relevance of the context it receives.\n\nRelevance is Key: Only provide information that is directly pertinent to the current task. Overloading the context window with irrelevant data can lead to confusion, increased token usage, and diminished performance.\nPrecision Matters: Ensure the information is accurate and unambiguous. Vague or contradictory context will lead to vague or contradictory responses.\nStructure for Clarity: Present context in a clear, consistent, and structured format. Use JSON, XML, or well-defined Markdown to organise information. This makes it easier for the LLM to parse and extract the necessary details.\nPrioritise Information: If context window limits are a concern, prioritise the most critical information. Techniques like summarisation or hierarchical retrieval can help manage large volumes of data.\n\n\n\n\nEffective memory management is fundamental to maintaining coherent and personalised interactions.\n\nShort-term Memory (Conversation History): For ongoing dialogues, implement strategies to manage conversation history. This might involve:\n\nSummarisation: Periodically summarising past turns to condense the history while retaining key information.\nWindowing: Keeping only the most recent N turns or a fixed number of tokens.\nHybrid Approaches: Combining summarisation with a sliding window.\n\nLong-term Memory (Knowledge Bases): For information that needs to persist across sessions or is too large for the context window, integrate with external knowledge bases. This typically involves:\n\nVector Databases: Storing embeddings of documents and retrieving relevant chunks based on semantic similarity (as seen in the RAG example).\nTraditional Databases: Storing structured data like user profiles, preferences, or past transactions.\nKnowledge Graphs: Representing complex relationships between entities for more sophisticated retrieval.\n\n\n\n\n\nTools extend the capabilities of your LLM, allowing it to interact with the real world and access up-to-date information.\n\nDefine Tool Capabilities Clearly: Ensure your tools have clear, concise descriptions that accurately convey their function to the LLM. The LLM relies on these descriptions to decide when and how to use a tool.\nHandle Tool Outputs: Design your graph to effectively process and integrate tool outputs back into the context. This might involve parsing JSON responses, summarising long text outputs, or handling errors gracefully.\nConditional Tool Use: Leverage LangGraph‚Äôs conditional edges to enable the LLM to dynamically decide whether a tool call is necessary based on the current query and context. This prevents unnecessary tool invocations and improves efficiency.\n\n\n\n\nContext engineering is rarely a one-shot process. It requires continuous refinement.\n\nStart Simple: Begin with a basic graph and gradually add complexity. Test each component and connection thoroughly.\nTest Edge Cases: Don‚Äôt just test happy paths. Actively try to break your system by providing ambiguous queries, unexpected inputs, or requests that require complex reasoning or tool use.\nMonitor Performance: Track metrics like response time, accuracy, and token usage. Identify bottlenecks or areas where context management can be improved.\n\n\n\n\nTools like LangSmith (from the creators of LangChain and LangGraph) are invaluable for debugging and understanding your LLM applications.\n\nTrace Agent Calls: LangSmith allows you to visualise the execution flow of your LangGraph application, showing you exactly what context was passed to each node, what the LLM‚Äôs thought process was, and what tool calls were made. This is critical for identifying where context might be failing or where the LLM is misinterpreting information.\nEvaluate Context Effectiveness: Use these traces to evaluate whether the context you are providing is indeed leading to the desired outcomes. Adjust your context engineering strategies based on these insights."
  },
  {
    "objectID": "posts/context-engineering-part-3/index.html#conclusion-the-future-is-context-aware-ai",
    "href": "posts/context-engineering-part-3/index.html#conclusion-the-future-is-context-aware-ai",
    "title": "Context Engineering: Building Smarter AI Agents - Part 3/3",
    "section": "Conclusion: The Future is Context-Aware AI",
    "text": "Conclusion: The Future is Context-Aware AI\nIt‚Äôs clear that the future of AI development, particularly with LLMs, lies beyond simple prompting. It resides in the meticulous design and management of the information environment in which these powerful models operate.\nContext engineering is not merely an optimisation; it is a fundamental shift in how we build intelligent systems. By actively curating, structuring, and dynamically managing the context, we empower LLMs to move from being reactive text generators to proactive, reasoning agents capable of tackling complex, real-world problems with unprecedented accuracy and reliability.\nLangGraph stands out as an indispensable framework in this new paradigm. Its graph-based architecture provides the necessary control and flexibility to orchestrate intricate workflows, manage state across multiple turns, integrate external tools seamlessly, and facilitate sophisticated multi-agent collaborations. It transforms the abstract challenge of context management into a concrete, programmable solution, enabling developers to build truly robust and context-aware AI applications.\nAs you embark on your journey to build more intelligent and capable LLM applications, remember that the key to unlocking their full potential lies in mastering context. Embrace context engineering, leverage the power of frameworks like LangGraph, and contribute to the exciting future of AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Hey there!",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJun 27, 2025\n\n\nContext Engineering: Building Smarter AI Agents - Part 3/3\n\n\n\n\nJun 20, 2025\n\n\nContext Engineering: Building Smarter AI Agents - Part 2/3\n\n\n\n\nJun 13, 2025\n\n\nContext Engineering: Building Smarter AI Agents - Part 1/3\n\n\n\n\nMay 18, 2025\n\n\nMCP Explained: The Bridge Between AI and the Real World\n\n\n\n\nMay 2, 2025\n\n\nHow LangGraph Uses Redis for Fault-Tolerant Task Execution\n\n\n\n\nApr 25, 2025\n\n\nHow to Build and Deploy an AI Agent using LangGraph from Scratch\n\n\n\n\n\nNo matching items"
  }
]