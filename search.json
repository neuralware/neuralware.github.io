[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm Prashant Patel, an AI/ML Engineer with a love for building clean, thoughtful, and scalable systems. I‚Äôve worked across both consultancies and large product companies, which has given me a strong foundation in system integration, solution architecture, and the many hats an AI Engineer ends up wearing - be it MLOps, backend development, or designing intelligent solutions end-to-end.\nI approach software design like building with LEGO-structured, modular, and purposeful. To me, writing code is a quiet act of communication with the future; it should be as clear and intentional as a well-written message to the next developer who picks it up. I enjoy thinking at abstract levels and solving real-world problems with elegance and simplicity. üå±\nA couple of years ago, I started neuralware. - a name that reflects where I believe software is headed: systems powered not just by logic, but by learning. It‚Äôs rooted in a long-standing fascination with neural networks and the idea that software is evolving into something more organic, more adaptable.\nReading Building a Second Brain by Tiago Forte inspired me to begin sharing my knowledge through this blog. We now live in an era where AI models are trained on our digital footprints - our code, our thoughts, our documentation. I believe that to shape a truly insightful oracle for the future, we need to feed it new, thoughtful, and human content - not just recycled history. It struck me how timely it is to contribute something meaningful to the growing collective knowledge.\nI naturally learn by doing - constantly experimenting, absorbing new ideas, and refining my understanding. This process of creation and iteration resonates deeply with the idea that we only know what we make, as Giambattista Vico so eloquently put it:\n\n‚ÄúVerum Ipsum Factum‚Äù\n‚Äì Giambattista Vico, an Italian Philosopher\n\nOutside of work, you‚Äôll find me either chasing after my toddler, getting lost in a good book, or immersing myself in platformer games on my Switch. I‚Äôm not one for spending much time outdoors - unless there‚Äôs a mountain involved. While I‚Äôm no professional climber, I am always up for scaling heights with good company to enjoy the views from the top; I find it the best way to disconnect and refresh.\n\n\n\nClimbing Cat Bells with my wife (Lake District, Cumbria)"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html",
    "href": "posts/langgraph-deployment/index.html",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "",
    "text": "Building and deploying a LangGraph AI Agent from scratch involves understanding the framework‚Äôs architecture, defining your agent‚Äôs workflow as a graph, implementing nodes and state management, and finally deploying the agent either locally or on LangGraph Cloud. Below is a detailed guide covering all these steps.\nBefore we dive-in, let‚Äôs get ourselves familiarised with some components involved in developing and deploying a LangGraph application."
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#components",
    "href": "posts/langgraph-deployment/index.html#components",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Components",
    "text": "Components\n\nLangGraph is the foundational library enabling agent workflow creation in Python and JavaScript.\nLangGraph API wraps the graph logic, managing asynchronous tasks and state persistence, serving as the backend engine.\nLangGraph Cloud hosts the API, providing deployment, monitoring, and accessible endpoints for running graphs in production.\nLangGraph Studio is the development environment that leverages the API backend for real-time graph building and testing, usable locally or in the cloud.\nLangGraph SDK offers programmatic access to LangGraph graphs, abstracting whether the graph is local or cloud-hosted, facilitating client creation and workflow execution.\n\n\n\n\n\n\n\nFigure¬†1: LangGraph Components"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#set-up",
    "href": "posts/langgraph-deployment/index.html#set-up",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Set Up",
    "text": "Set Up\nWe‚Äôll be building a simple Agent to demonstrate the end-to-end process. Building more sophisticated AI agents is a topic better suited for a dedicated post.\nTo start with, create a following project structure and open langgraph_deployment directory in your favorite code editor.\nlanggraph_deployment/\n‚îú‚îÄ‚îÄsrc\n‚îÇ   ‚îú‚îÄ‚îÄchat_agent/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nodes.py   # Node functions\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py   # State definitions\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py       # Graph construction code\n‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ main.py                # LangGraph SDK demo\n‚îú‚îÄ‚îÄ .env                   # Environment variables (add OPENAI_API_KEY)\n‚îú‚îÄ‚îÄ .python-version        # Python version for this project (3.11)\n‚îî‚îÄ‚îÄ langgraph.json         # LangGraph API configuration\n\nInstall dependencies\nWe will be using uv for dependency management, if you haven‚Äôt used uv package manager before - now is the good time to start. You can install uv by following instructions on their official page.\nOnce installed run the below commands to set up the python environment.\nuv python install 3.11\nuv init\nOn successful completion of above commands, you will see pyproject.toml added to the project. This is uv equivalent for the requirements.txt for managing project dependencies and bundling the project.\nAppend following lines to pyproject.toml. This is so that uv recognizes the build system and can build and install your package into the project environment.\n\n\npyproject.toml\n\n[build-system]\nrequires = [\"setuptools&gt;=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\n\nFinally, install the dependencies.\nuv add \"langchain[openai]\" \"langgraph-cli[inmem]\" \\\n        langgraph langgraph-api langgraph-sdk"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#build",
    "href": "posts/langgraph-deployment/index.html#build",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Build",
    "text": "Build\nTime to get our hands dirty (in a good way)!\nWe will create a marketing research AI assistant using OpenAI‚Äôs gpt-4o model. Since we won‚Äôt be using any tools or any special patterns, our graph will be simple (intentionally). Let‚Äôs start adding code to the files we created.\n\nDefine the Agent State\nLangGraph uses a shared state object to pass information between nodes. Define the state using Python‚Äôs TypedDict to specify the data structure.\n\n\nstate.py\n\nfrom operator import add\nfrom typing import Annotated, TypedDict, Any\nfrom langchain_core.messages import AnyMessage\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]  # Accumulates messages\n\nThis state holds a list of messages (e.g., user input, AI responses) that are appended to as the conversation proceeds.\n\n\nImplement Node Functions\nNodes are the processing units of your graph. Each node is a function that takes the current state as input and returns an updated state. We will add a system message to instruct the model to follow a specific role, tone, or behavior during its execution within the node.\n\n\nnodes.py\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import SystemMessage\n\nfrom .state import State\n\n_ = load_dotenv()\n\nmodel = init_chat_model(model=\"gpt-4o\", model_provider=\"openai\")\n\ninstructions = (\n    \"You are a Marketing Research Assistant. \"\n    \"Your task is to analyze market trends, competitor strategies, \"\n    \"and customer insights. \"\n    \"Provide concise, data-backed summaries and actionable recommendations. \"\n)\nsystem_message = [SystemMessage(content=instructions)]\n\n\ndef chat(state: State) -&gt; dict:\n    messages = system_message + state[\"messages\"]\n    message = model.invoke(messages)  # Generate response\n    return {\"messages\": [message]}  # Return updated messages\n\n\n\nConstruct the Graph Workflow\nUse LangGraph‚Äôs StateGraph to build your agent‚Äôs workflow by adding nodes and edges that define the flow of execution.\n\n\nagent.py\n\nfrom langgraph.graph import END, START, StateGraph\nfrom utils.nodes import chat\nfrom utils.state import State\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chat\", chat)\n\ngraph_builder.add_edge(START, \"chat\")\ngraph_builder.add_edge(\"chat\", END)\n\ngraph = graph_builder.compile()"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#bundle",
    "href": "posts/langgraph-deployment/index.html#bundle",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Bundle",
    "text": "Bundle\nTypically, you‚Äôd need to write a backend to expose your application as an API. However, with LangGraph, that entire step is streamlined - simply add a config file and launch the service.\n\nLangGraph API Config\nTo configure the API, define the settings in langgraph.json by adding the following lines. The configuration is straightforward, so we won‚Äôt go into the details of each field here. However, if you‚Äôre interested in exploring advanced configuration options, you can refer to the official documentation here.\n\n\nlanggraph.json\n\n{\n    \"graphs\": {\n        \"agent\": \"chat_agent/agent.py:graph\"\n    },\n    \"env\": \".env\",\n    \"python_version\": \"3.11\",\n    \"dependencies\": [\n        \".\"\n    ]\n}\n\n\n\nLaunch Service\nOnce your configuration is in place, use the following command to bundle the application and launch it as a web service:\nlanggraph dev\nThis command not only generates an API with ready-to-use endpoints but also spins up a web-based chat interface (Studio UI) that makes it easy to test and debug your application in real time.\nüöÄ API: http://127.0.0.1:2024\nüé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\nüìö API Docs: http://127.0.0.1:2024/docs"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#deploy",
    "href": "posts/langgraph-deployment/index.html#deploy",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Deploy",
    "text": "Deploy\nAt this point, we have the LangGraph server running locally. Now, it‚Äôs time to deploy it. As shown in the component diagram, there are three primary deployment options available:\n\nCloud SaaS: Deploy LangGraph Servers to LangChain‚Äôs managed cloud infrastructure.\nSelf-hosted: Host and manage the LangGraph infrastructure within your own cloud environment - either just the data plane (with LangChain maintaining the control plane), or fully self-host both for complete control.\nStandalone container: Package the LangGraph Server as a Docker image and deploy it wherever you like - such as a Kubernetes cluster - while connecting to separately hosted Postgres and Redis instances.\n\nEach option comes with trade-offs:\n\nOption 1 requires the least setup but involves commercial licensing and vendor lock-in.\nOption 2 demands the most effort in terms of design, setup, and ongoing management, but offers maximum control.\nOption 3 strikes a balance, offering flexibility in deployment without full platform dependency.\n\nAlthough there are trade-offs either option can be selected based on your internal evaluation and convenience. We will explore the Option 3 - Standalone Container.\n\nStandalone Container\nLangGraph supports standalone container deployments using Docker, making it possible to run a scalable, self-managed service - ideal for deploying to environments like Kubernetes. This option is non-commercial and gives you full control over the infrastructure.\nWhile we won‚Äôt be deploying to a Kubernetes cluster in this guide, we‚Äôll simulate the setup locally using Docker to mirror the architecture shown in the diagram below. Specifically, we‚Äôll deploy three core services:\n\nLangGraph API ‚Äì the same service we configured and tested locally in the previous step.\nPostgreSQL ‚Äì used for persistent storage by LangGraph.\nRedis ‚Äì used for task orchestration and streaming output events through Pub/Sub.\n\nThis local deployment will serve as a lightweight, production-like replica, allowing us to validate the architecture and interactions between components without the overhead of a full Kubernetes setup.\n\n\n\n\n\n\nFigure¬†2: Standalone Container Deployment\n\n\n\n\n\nLocal Deployment with Docker Compose\nTo simplify service orchestration, we‚Äôll use Docker Compose to bring up the LangGraph API along with its required dependencies - PostgreSQL and Redis. This setup replicates a production-like environment locally and abstracts away the need to manage containers individually.\n\nLangGraph API\nIf the application is structured correctly, you can build a docker image with the LangGraph Deploy server.\nlanggraph build --tag chat_agent:v1.0.0\nThis command packages your application into a Docker image named chat_agent:v1.0.0, ready for deployment.\n\n\nDocker Compose\nWe‚Äôll define the services in a docker-compose.yml file as shown below.\n\n\ndocker-compose.yml\n\nvolumes:\n  langgraph-data:\n    driver: local\n\nservices:\n  langgraph-redis:\n    image: redis:6\n    healthcheck:\n      test: [ \"CMD\", \"redis-cli\", \"ping\" ]\n      interval: 5s\n      timeout: 1s\n      retries: 5\n\n  langgraph-postgres:\n    image: postgres:16\n    ports:\n      - \"5433:5432\"\n    environment:\n      POSTGRES_DB: postgres\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n    volumes:\n      - langgraph-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [ \"CMD\", \"pg_isready\", \"-U\", \"postgres\" ]\n      start_period: 10s\n      timeout: 1s\n      retries: 5\n      interval: 5s\n\n  langgraph-api:\n    image: ${IMAGE_NAME}\n    ports:\n      - \"8123:8000\"\n    depends_on:\n      langgraph-redis:\n        condition: service_healthy\n      langgraph-postgres:\n        condition: service_healthy\n    env_file:\n      - .env\n    environment:\n      REDIS_URI: redis://langgraph-redis:6379\n      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}\n      POSTGRES_URI: postgres://postgres:postgres@langgraph-postgres:5432/postgres?sslmode=disable\n\n\n\nRunning the Application\nEnsure that your .env file includes valid values for IMAGE_NAME and LANGSMITH_API_KEY before running the Docker Compose setup.\n\n\n.env\n\nOPENAI_API_KEY=your-openai-api-key\nLANGSMITH_API_KEY=your-langsmith-api-key\nIMAGE_NAME=chat_agent:v1.0.0\n\nOnce everything is set, start the stack by running:\ndocker compose up --build\nThis will -\n\nStart and link the required Redis and PostgreSQL containers\nBuild and launch your LangGraph API container\nWait for dependent services to become healthy before launching the API\nExpose the API locally at http://localhost:8123\n\nYou can test that the application is up by checking:\ncurl --request GET --url 0.0.0.0:8123/ok\nAssuming everything is running correctly, you should see a response like:\n{\"ok\":true}"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#interact",
    "href": "posts/langgraph-deployment/index.html#interact",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Interact",
    "text": "Interact\nIf you‚Äôve made it this far - congratulations! üéâ\nThe final step is to interact with your deployed LangGraph server using the LangGraph SDK.\nLangGraph provides SDKs for both Python and JavaScript/TypeScript, making it easy to work with deployed graphs - whether hosted locally or in the cloud. The SDK abstracts away the complexity of direct API calls and provides:\n\nA unified interface to connect with LangGraph graphs\nEasy access to assistants and conversation threads\nSimple run execution and real-time streaming capabilities\n\nFollowing is the code-snippet in Python to demonstrate interaction with the graph we deployed in the previous step -\n\n\nmain.py\n\nimport asyncio\n\nfrom langchain_core.messages import HumanMessage\nfrom langgraph_sdk import get_client\n\nLANGGRAPH_SERVER_URL = \"http://localhost:8123\"\n\nasync def main() -&gt; None:\n\n    # Initialize the LangGraph client\n    client = get_client(url=LANGGRAPH_SERVER_URL)\n\n    # Fetch the list of available assistants from the server\n    assistants = await client.assistants.search()\n\n    # Select the first assistant from the list\n    agent = assistants[0]\n\n    # Create a new conversation thread with the assistant\n    thread = await client.threads.create()\n\n    # Prepare the user message\n    user_message = {\"messages\": [HumanMessage(content=\"Hi\")]}\n\n    # Stream the assistant's response in real-time\n    async for chunk in client.runs.stream(\n        thread_id=thread[\"thread_id\"],\n        assistant_id=agent[\"assistant_id\"],\n        input=user_message,\n        stream_mode=\"values\",\n    ):\n        # Filter out metadata events and empty data chunks\n        if chunk.data and chunk.event != \"metadata\":\n            # Print the content of latest assistant message\n            print(chunk.data[\"messages\"][-1][\"content\"])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nThe above code is intuitive, so I‚Äôll leave it to you to run and explore.\nAt first glance, the response you get might feel modest - a simple message in return for a simple prompt. But behind that is a clean, scalable setup that mirrors how production-grade AI systems are built. You‚Äôve laid the groundwork for something much bigger. Swapping out this basic agent for a more advanced one is just a matter of choice now. So while it might not feel flashy, make no mistake - you‚Äôve just put a serious system in place.\n\nWant to take this further? Try integrating it into a Streamlit frontend to create an interactive, real-time chat experience."
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#further-reading",
    "href": "posts/langgraph-deployment/index.html#further-reading",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "Further Reading",
    "text": "Further Reading\n\nHow to Deploy to Cloud SaaS\nSelf-Hosted Data Plane\nSelf-Hosted Control Plane\nHow to add custom authentication\nHow to add custom routes\nHow to integrate LangGraph into your React application\nLangGraph SDK"
  },
  {
    "objectID": "posts/langgraph-deployment/index.html#references",
    "href": "posts/langgraph-deployment/index.html#references",
    "title": "How to Build and Deploy an AI Agent using LangGraph from Scratch",
    "section": "References",
    "text": "References\n\nLangGraph Deployment Options\nHow to do a Self-hosted deployment of LangGraph\nHow to Deploy a Standalone Container"
  },
  {
    "objectID": "posts/langgraph-redis/index.html",
    "href": "posts/langgraph-redis/index.html",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "",
    "text": "In How to Build and Deploy an AI Agent using LangGraph from Scratch, we deployed an AI agent to a standalone container running the LangGraph server. This setup communicates with both PostgreSQL and Redis. In that architecture, PostgreSQL serves as a checkpointer - responsible for persisting agent state across interactions, allowing the agent to ‚Äúremember‚Äù previous conversations.\nHowever, the role of Redis may have felt less clear. It‚Äôs briefly described as a task orchestrator and a channel for streaming output, but beyond that, LangGraph‚Äôs documentation doesn‚Äôt dive deep into its purpose or inner workings.\nOne of the advantages of using open-source software is transparency - we can inspect the implementation to truly understand what‚Äôs going on under the hood. So that‚Äôs exactly what I did. 1\nIn this article, we‚Äôll explore how Redis fits into the LangGraph architecture and how it plays a crucial role in enabling fault-tolerant, stateful conversational AI applications, especially when working alongside PostgreSQL and the LangGraph server."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#redis-in-langgraph",
    "href": "posts/langgraph-redis/index.html#redis-in-langgraph",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Redis in LangGraph",
    "text": "Redis in LangGraph\nAt its core, Redis serves as LangGraph‚Äôs high-performance messaging backbone, enabling two critical capabilities:\n\nTask queuing with retry counts for fault-tolerant agent workflows\nReal-time streaming of intermediate agent outputs\n\nUnlike traditional databases, Redis specializes in low-latency operations through its in-memory data structures. For LangGraph, this means:\n\nRedis List act as FIFO queues for agent task scheduling\nRedis String and Pub/Sub for bi-directional signaling (output streaming / cancellations)\nBlocking Queues create reliable queues that survive worker crashes\n\nThese concepts will be clear once we start looking into the internal working.\n\n\n\n\n\n\nNote\n\n\n\nBy using Redis as a message broker, LangGraph decouples the components that produce runs (agents/servers) from the workers that execute them.\n\n\n\nDuality\nA key architectural advantage is Redis‚Äô ability to function as both a queue and a broadcast system simultaneously. When your LangGraph agent processes a request:\n\nPostgreSQL persists the final state (the ‚Äúwhat‚Äù)\nRedis orchestrates the execution path (the ‚Äúhow‚Äù)\n\nThis explains why LangGraph requires both databases - while PostgreSQL provides durability, Redis delivers the coordination layer that makes stateful, long-running agent workflows possible."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#under-the-hood",
    "href": "posts/langgraph-redis/index.html#under-the-hood",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Under the hood",
    "text": "Under the hood\nThe following sequence diagram captures the key interactions and the role of Redis as a messaging and signaling layer, with Postgres as the durable run data store.\n\n\n\n\n\n\nFigure¬†1: How LangGraph uses Redis\n\n\n\n\nExplanation\nThe Agent creates a new run by inserting it into Postgres and signals workers2 via a sentinel3 pushed to a Redis list.\n\nThe Worker blocks on Redis list (BLPOP4) waiting for a wake-up signal.\nUpon receiving the signal, the worker fetches the actual run data from Postgres.\nThe worker executes the run asynchronously.\nDuring execution, the worker streams output events via Redis PubSub to the agent.\nIf a cancellation is requested, the agent sets a cancellation flag in Redis, which is communicated to the worker via PubSub channel.\nUpon completion, the worker updates the run status in Postgres, clears ephemeral metadata in Redis, and notifies the agent (e.g., via webhook)."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#crash-resilience",
    "href": "posts/langgraph-redis/index.html#crash-resilience",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Crash Resilience",
    "text": "Crash Resilience\nAt the heart of LangGraph‚Äôs fault-tolerance lies its use of Redis lists as a transactional queue. This is achieved via BLPOP and Atomic Task Handoffs.\n\nCrash-Safe Task Claiming\nWhen a worker executes a BLPOP tasks: queue 0 (as shown in the sequence diagram), the operation is atomic5:\n\nRedis only removes the task from the list after it has been delivered to the worker.\nIf the worker crashes before processing the task, the task remains in the queue.\n\nThis ensures that no task is lost due to unexpected crashes or restarts. Additionally, LangGraph sets a sensible default for concurrency control:\nN_JOBS_PER_WORKER = env(\"N_JOBS_PER_WORKER\", cast=int, default=10)\nEach worker will process up to 10 tasks in parallel unless configured otherwise, helping scale horizontally while keeping workloads isolated.\n\n\nThe Sentinel Pattern\nIn the diagram, you may have noticed agents pushing a sentinel - a wake-up signal - into Redis.\n\n\n\n\n\n\nFigure¬†2: The Sentinel Pattern\n\n\n\nHere‚Äôs how it works:\n\nWorkers use BLPOP with a timeout of 0, meaning they block indefinitely until a signal arrives.\nThis design eliminates the need for polling and ensures no messages are missed - even if a worker restarts.\n\nLangGraph also reports internal metrics periodically to help with observability:\nSTATS_INTERVAL_SECS = env(\"STATS_INTERVAL_SECS\", cast=int, default=60)\nThis allows systems to track worker health and performance at one-minute intervals by default.\n\n\nPostgreSQL as a Fallback\nIf Redis is temporarily unavailable or restarts, LangGraph falls back to PostgreSQL via Runs.next, as shown in the query step of the diagram.\nThis creates a two-layered recovery mechanism:\n\nRedis-first (hot path): Fast, in-memory task delivery.\nPostgreSQL (cold path): Durable, persistent task storage.\n\nThis layered approach ensures resilience without sacrificing performance. LangGraph also sets an upper bound on how long a background task can run:\nBG_JOB_TIMEOUT_SECS = env(\"BG_JOB_TIMEOUT_SECS\", cast=float, default=3600)\nThis ensures that long-running or stuck jobs don‚Äôt hang forever - by default, any job exceeding 1 hour is forcefully timed out.\n\n\nWhy This Beats Polling\nPolling-based systems often suffer from subtle timing issues. For instance, if a worker uses RPOP6 to dequeue a task and then crashes before it begins processing, that task is effectively lost. Similarly, network hiccups or buffering delays can cause tasks to be silently dropped or missed altogether.\nLangGraph avoids these pitfalls by using BLPOP, which blocks server-side until a task is available. This means tasks remain in Redis until they are actually handed off to a live worker. There are no polling loops, no race conditions, and no dependency on fragile network timing - just clean, atomic task delivery."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#putting-it-all-together",
    "href": "posts/langgraph-redis/index.html#putting-it-all-together",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Putting it All Together",
    "text": "Putting it All Together\nIn summary, LangGraph cleverly integrates Redis not just as a simple cache, but as a core component for resilient task orchestration and real-time communication. By leveraging atomic operations like BLPOP for crash-safe task claiming and Pub/Sub for efficient signaling and output streaming, it builds a robust system that complements PostgreSQL‚Äôs role in state persistence. This dual-database approach allows LangGraph to deliver performant, fault-tolerant execution for complex, stateful AI agent workflows, ensuring tasks are reliably processed even amidst potential failures."
  },
  {
    "objectID": "posts/langgraph-redis/index.html#references",
    "href": "posts/langgraph-redis/index.html#references",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "References",
    "text": "References\n\nRedis Queue\nLangGraph: How we use Redis"
  },
  {
    "objectID": "posts/langgraph-redis/index.html#footnotes",
    "href": "posts/langgraph-redis/index.html#footnotes",
    "title": "How LangGraph Uses Redis for Fault-Tolerant Task Execution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLangGraph doesn‚Äôt publish source code for langgraph-storage, but only wheels. So you have to install the package to view the code. The implementation is present in langgraph_storage.queue‚Ü©Ô∏é\nWorkers in this context are async tasks in Python, they have nothing to do with Redis.‚Ü©Ô∏é\nA sentinel is a special placeholder or signal value used within a Redis list to notify workers - it has no actual run info. Not to be confused with Redis Sentinel (the high-availability system).‚Ü©Ô∏é\nIn a basic queue, if a consumer tries to dequeue a task when the queue is empty, it gets a null response and may need to poll the queue repeatedly. To avoid this, Redis provides a way to implement blocking queues. In a blocking queue, if a consumer tries to dequeue a task when the queue is empty, it is put to sleep by Redis until a task is available.‚Ü©Ô∏é\nAtomic tasks means, they either complete fully or not at all‚Ü©Ô∏é\nRPOP: Redis command to dequeue an element (FIFO)‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Hey there!",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nMay 2, 2025\n\n\nHow LangGraph Uses Redis for Fault-Tolerant Task Execution\n\n\n\n\nApr 25, 2025\n\n\nHow to Build and Deploy an AI Agent using LangGraph from Scratch\n\n\n\n\n\nNo matching items"
  }
]